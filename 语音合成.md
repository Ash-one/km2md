# 语音合成系统
## TTS的实现
### 文本分析
#### 按层次划分
##### 音素级别
###### 常常单独表示TTS中的文本
##### 音节级别
##### 单词级别
##### 短语级别
##### 句子级别
#### 目的
##### 提取语言特征
###### 包含有关发音和韵律的丰富上下文信息
#### 过程
##### 文本规范化
###### 目的
####### 将原始书面文本转换为口语形式的单词
######## 1989年->一九八九年
###### 方法
####### 基于规则
####### 神经网络
######## 源序列为非标准单词，目标序列为口语形式单词
####### 规则与神经模型结合
######## Zhang等人通过将基于传统的规则系统与由Transformer中的多头自注意模块组成的神经网络，从而标准化普通话文本。预测准确性更高
##### 分词
###### 检测单词边界
##### 词性标记
##### 韵律预测
###### 韵律信息
####### 节奏
######## 对应音节持续时间
####### 重音
######## 对应响度
####### 语音语调
######## 对应音高变化
###### 层次
####### 中文基于韵律边界标签
######## 韵律词PW
######## 韵律短语PPH
######## 语调短语IPH
###### 方法
####### 参考编码器，从参考语音中学习韵律表示
####### 自监督预训练学习具有隐韵律信息的良好文本表示的文本
####### 专用建模（图网络）合并语法信息
####### Lu提出了基于自注意力模型的多任务学习方法
##### 字素到音素转换
###### 中文基于词典,pypinyin
####### 字素到音素可直接转换
####### 多音字需要根据字符上下文决定
####### 特殊情况
######## 儿化音
######## 连续三声变化
######## 轻声
######## 停顿符号：，、；,
##### 可添加风格信息
###### 将话语的前后文和呼吸发音时间作为数据集
##### 可提取上下文信息
### 声学模型
#### 目的
##### 从语言特征提取声学特征
###### 非端到端常用特征
####### LSP（line spectral pairs）
####### MCC(mel-cepstral coefficients)
####### MGC(mel-generalized coefficients)
####### F0
####### BAP(band aperiodicities)
###### 端到端常用特征
####### MelS(mel-spectrogram)
####### LinS(linear-spectrograms)
#### 阶段
##### 统计参数语音合成SPSS
###### 基于HMM
####### 优点：可以灵活改变说话者身份、情绪和说话风格更灵活
####### 缺点：声学模型精度不好，预测的声学特征过度平滑，缺乏细节；声码技术不够好
###### 基于DNN
####### 提高了合成质量
###### LSTM
####### 模拟上下文依赖性
###### RNN
###### GAN
##### 端到端语音合成
###### 优点：预处理更少
###### 方式
####### 基于RNN
######## Tacotron系列
######### Tacotron将字符序列作为输入并输出LinS，使用Griffin-Lim生成波形
########## 使用CBHG和GRU
######### Tacotron2生成MelS，使用WaveNet作为声码器生成波形
########## 使用LSTM
########## 优点：相比旧方法大大提高了语音质量
######## 改进Tacotron
######### GST-Tacotron
########## 使用参考编码器和样式标记增强表现力
######### DurIAN和无注意力Tacotron
########## 去除了注意力机制，使用持续时间预测器进行自回归预测
######### Parallel Tacotron 1/2
########## 将自回归生成改为非自回归生成
######### Wave-Tacotron
########## 文本到波形模型
######## 存在问题
######### 循环要求RNN编码解码器不能并行训练，编码器在推理中不能并行，影响效率
######### 文本和语音序列很长，RNN不擅长对序列中的长依赖关系建模
####### 基于CNN
######## DeepVoice系列
######### DeepVoice是一个用CNN增强的SPSS系统，神经网络得到语言特征后，利用WaveNet声码器合成波形
######### DeepVoice2改进网络结构和多说话人建模，利用Tacotron生成LinS，使用WaveNet生成波形
######### DeepVoice3完全使用CNN，从字符生成MelS
######## ClariNet
######## ParaNet
######## DCTTS
######### 利用CNN从字符序列生成MelS，使用频谱图超分辨率网络获得LinS，使用Griffin-Lim合成波形
####### 基于Transformer
######## FastSpeech系列
######### FastSpeech
########## Transformer模型能够并行生成MelS
########## 消除文本和语音之间的注意力机制，避免单词跳过和重复的问题
########## 使用长度调节器来弥合音素与MelS序列的长度不匹配
######### FastSpeech2 
########## 使用ground-truthMelS作为训练目标，避免信息丢失
########## 用更多的方差信息，如音高、持续时间、能量作为解码器输入，缓解了文本到语音转换中的一对多映射问题
######## TransformerTTS
######### 实现了和Tacotron2类似的语音质量，但训练时间更短
######### 存在问题
########## 并行计算被认为不可靠，建议增加鲁棒性
######### 改进
########## RobuTrans
########### 利用持续时间预测增强自回归生成的鲁棒性
####### 其他声学模型
######## Flow
######## GAN
######## VAE
######## Diffusion
### 语音合成Vocoder
#### SPSS声码器
##### STRAIGHT
##### WORLD
###### 步骤
####### 声码器分析
######## 分析语音并获得声学特征
####### 声码器合成
######## 从特征生成波形
#### 神经网络声码器
##### 自回归声码器
###### WaveNet
###### SampleRNN
###### LPC-Net
####### 将数字信号处理引入神经网络
##### 基于Flow的声码器
###### 自回归变换AF
###### 二分变换
##### 基于GAN的声码器
###### 不能估计数据样本的可能性
##### 基于VAE的声码器
##### 基于Diffusion的声码器
### 完全端到端
#### 优点
##### 较少的人工注释和特征开发
##### 避免级联模型中的误差传播
##### 降低训练开发和部署的成本
#### 难点
##### 文本与语音波形之间的模态不同
##### 字符/音素序列与波形序列之间巨大的长度不匹配
#### 模型
##### FastSpeech2s
##### EATS
##### Wave-Tacotron
##### VITS
##### NaturalSpeech
## TTS中的高级主题
### 快速
#### 并行生成
##### 基于RNN的自回归模型在训练和推理上都很慢，O(N)的训练和推理时间复杂度
##### CNN或基于自注意机制的结构可以支持并行训练，但仍需自回归推理O(1),O(N)
###### FastSpeech1/2使用前馈transformer的自注意结构并行训练和非自回归推理O(1),O(1)
##### 基于GAN的模型都是非自回归的，O(1)O(1) 
##### 基于Flow的模型能利用生成流并行训练推理，需要堆叠多个流迭代T，O(T)O(T)
#### 轻权重模型
##### 目的
###### 在手机或嵌入式设备上部署，降低模型参数的数量和计算成本
##### 技术
###### 修剪
###### 量化
###### 知识蒸馏
###### 神经架构搜索
#### 利用语音领域知识加速
##### 线性预测
###### LPC-Net
####### 使用线性预测系数来计算下一个波形，并使用轻量级模型来预测残值
##### 多频带模型
###### Bunched LPCNet
####### 通过sample bunching和bit bunching加速两倍以上
##### 模型预测
##### 多帧预测
##### 流合成
### 低资源
#### 自监督训练
##### 针对未配对的文本和语音，利用自监督预训练方法增强语言理解或生成能力
###### 文本编码器可以通过预先训练的BERT模型增强
###### 语音解码器可以通过自回归MelS预测进行预训练
###### 语音转换任务联合训练
#### 跨语言转换
##### 在资源丰富的语言上预先训练模型以帮助低资源语言的文本和语音质检的映射
#### 跨发言人转换
##### 当某个说话人的语音数据有限时，可以利用来自其他说话者的数据来提高该说话者的合成质量。这可以通过语音转换将其他说话者的语音转换为该目标语音来实现，以增加训练数据，或者通过语音适应或语音克隆将其他语音训练的TTS模型调整为该目标语音
#### 语音链/反向转换
##### 与自动语音识别结合互相改进，利用未配对的文本和语音数据提高性能
#### 野生数据集挖掘
##### 网络中可能存在一些低质量的文本和语音配对数据，挖掘这类数据并开发复杂的技术来训练TTS模型。语音增强，去噪和disentangling可以用来提高挖掘的语音数据的质量。
### 鲁棒性
#### 问题
##### 学习字符/音素和MelS之间的对齐方式困难
##### 自回归生成中出现的曝光偏差和误差传播问题
###### 曝光偏差
####### 序列生成模型通常通过将先前的ground-truth值作为输入（即教师强迫）来训练，但通过将先前的预测值作为推理中的输入来自动回归生成序列。
###### 误差传播
####### 训练和推理之间的不匹配可能导致推理中的错误传播，其中预测误差可以沿着生成的序列快速累积。
##### 出现在神经TTS中，声码器没有严重的鲁棒性问题，因为声学特征和波形已经按帧对齐
#### 解决问题1
##### 增强注意力
###### 属性
####### 局部性
######## 一个字符/音素标记对齐到一个或多个连续的MelS帧，而一个MelS帧只能对齐到单个字符/音素标记，避免模糊注意力与注意力崩溃
####### 单调性
######## 如果字符A在B后面，则对应A的MelS帧也位于B的MelS帧后面，避免单词重复
####### 完整性
######## 每个字符/音素消极必须至少被一个MelS帧覆盖，避免跳过单词
###### 基于内容的注意力
###### 基于位置的注意力
####### 考虑文本与语音之间的对齐方式取决于位置
###### 基于内容位置的混合注意力
####### 在计算当前注意力对齐时，使用先前的注意力对齐
###### 单调的注意力
####### 利用文本和语音之间的对齐时单调的先验，注意力位置是单调增加的，避免跳过和重复的问题
####### 逐步单调注意力，在每个解码步骤中，注意对齐位置最多向前移动一步，并且不允许跳过任何输入单元
###### 窗口化或偏离对角线的惩罚
####### 将对源序列的注意力限制为窗口子集
####### 对没在对角线的注意力权重进行惩罚损失
###### 增强编码器-解码器连接
##### 使用持续时间duration取代注意力，弥合文本和语音之间的长度不匹配
###### 目的
####### 消除注意力，明确预测每个字符/音素的持续时间，并根据持续时间扩展文本隐藏序列以匹配MelS序列的长度，之后生成MelS序列
###### 使用外部对齐工具或联合训练来获得持续时间标签
####### 外部工具
######## 编码器-解码器注意力
######### 从自回归声学模型的注意力对其中获得持续时间标签
######### 从自回归教师模型中提取持续时间，用CNN取代整个网络结构
######## CTC对齐
######### 利用基于CTC的ASR模型提供音素和MelS序列之间的对齐
######## HMM对齐
######### 利用基于HMM的Montreal forced alignment来获得持续时间
####### 内部对齐
######## AlignTTS利用基于动态规划的方法，通过多阶段训练来学习文本和MelS序列之间的对齐关系
######## JDI-T从自回归教师模型中提取持续时间，但是共同训练自回归和非自回归模型，不用两阶段训练
###### 以端到端方式优化持续时间预测或在训练中使用ground-truth持续时间并预测推理的预测时间
####### 为了共同优化持续时间以实现更好的韵律，EATS 使用内部模块预测持续时间，并在持续时间插值和软DTW损失的帮助下端到端优化持续时间。
####### Parallel Tacotron 2 遵循EATS的实践，以确保可微分的持续时间预测。
####### Non-Attentive Tacotron 提出了一种用于持续时间预测的半监督学习，如果没有可用的持续时间标签，则预测的持续时间可用于上采样。
#### 解决问题2
##### 增强自回归
###### Guo等人利用教授强迫来缓解真实数据和预测数据不同分布之间的不匹配。
###### Liu等人进行师生蒸馏以减少暴露偏倚问题，其中教师接受教师强迫模式的训练，学生将先前预测的值作为输入并进行优化以减小教师和学生模型之间隐藏状态的距离。考虑到由于误差传播，生成的MelS序列的右侧部分通常比左侧部分更差，一些工作利用从左到右和从右到左的生成进行数据增强和正则化。
###### Vainer和Dušek 利用一些数据增强来缓解曝光偏差和误差传播问题，方法是为每个数据添加一些随机的高斯噪声
##### 使用非自回归取代自回归
### 富有表现力
#### 关键
##### 处理一对多映射问题，即同一文本在时长、音高、音量、说话人风格、情感等方面有多个语音变体对应。
#### 变体信息的分类
##### 文本信息
###### 字符
###### 音素
##### 说话人或音色信息
##### 韵律、风格或情感信息
###### 语调
###### 节奏
###### 重音
##### 录音设备或噪声环境
#### 建模变体信息
##### 信息类型
###### 显性信息
####### 直接作为输入
###### 隐性信息
####### 参考编码器
######## skry-ryan 从参考音频中提取韵律嵌入，并将其作为解码器的输入，训练时使用ground-truth参考音频，推理是使用另一个参考音频合成具有类似韵律的语音
####### 变分自编码器
######## zhang利用高斯先验作为正则化，利用vae对潜在空间中的方差信息进行建模，这可以实现表达建模，控制合成风格
####### 高级生成模型
######## 建模多模态分布，需要选择能够更好地模拟多模态分布的损失，如基于混合分布、SSIM、Flow、GAN或基于扩散的损失
####### 文本预训练
##### 信息粒度
###### 语言级别/说话人级别
####### 使用id区分语言或说话人
###### 段落级别
####### 需要考虑长形式阅读中的话语、句子之间的关系
###### 话语级别
####### 从参考语音中提取一个单独的隐藏向量来代表其timber/风格/韵律
###### 单词/音节级
####### 对话语级信息无法覆盖的细粒度风格/韵律信息进行建模
###### 字符/音素级别
####### 音长、音高或韵律信息
###### 帧级别
####### 最细粒度信息
###### 建模方式
####### kenter从框架和音素水平预测韵律特征到音节水平，并连接单词和句子水平特征
####### hono利用多粒度的VAE获取不同时间分辨率的潜变量，并从粗级别(例如，从话语级到短语级，再到单词级)抽取更细级别的潜变量
####### sun用VAE对音位级和词级的方差信息建模，并将它们组合在一起作为译码器的输入。
####### chien和lee提出了一个从词到音位的层次结构来提高韵律预测
#### Disentangling/ Controlling/Transferring
##### 通过对抗训练解缠
###### 目的
####### 风格和韵律纠缠在一起时，需要在训练中解缠
###### 方式
####### Ma等人通过对抗性和合作性游戏增强了内容风格的解缠能力和可控性
####### Hsu利用vae框架进行对抗性训练，从说话人信息中分离噪声
####### qian等人提出使用三种瓶颈重构来分离语音流的节奏、音高、内容和音色
####### Zhang等人提出通过帧级噪声建模和对抗性训练从说话人中分离噪声。
##### 控制的周期一致性/反馈损失
###### 目的
#######  当提供样式标签等变异信息作为输入时，TTS模型应该用相应的样式合成语音。如果不添加约束，TTS模型往往忽略方差信息和不遵循风格的合成语音。
####### 为了增强输入方差信息的可控性，一些研究提出了使用周期一致性或反馈来鼓励合成语音在输入中包含方差信息。
###### 方式
####### Li等人通过添加一个带有反馈循环的情感风格分类器来进行可控的情感迁移，鼓励TTS模型合成带有特定情感的语音
##### 控制的半监督学习
###### 当部分标签可用时，Habib等人提出了半监督学习方法来学习VAE模型的潜变量，以控制情感或语率等属性。
###### 当没有可用的标签时，Hsu等人提出使用高斯混合VAE模型解纠缠不同的属性，或使用梯度反转或对抗训练解缠说话人和噪声，可以为有噪声的说话人合成纯净语音。
##### 变更转移variance信息
###### 在已标注的标签中提供variation信息
####### 在训练中使用该语音和相应的标签，在推理中传递带有不同标签的语音风格
###### 没有标注的标签作为variation信息
####### 从参考语音中提取
####### 从文本预测
####### 从潜空间取样获得
### 适应性
#### 别名
##### 语音适应
##### 语音克隆
##### 自定义语音
#### 方向
##### 一般适应性
###### 源模型泛化
####### 目的
######## 提高源模型的可生成性
####### 方式
######## chen提出根据声学条件建模而不是记忆，提供必要的声学信息
######### 韵律
######### 说话人音色
######### 录音环境
######## 增加说话人数量，增强泛化能力
###### 跨域的适应
####### AdaSpeech3可以将阅读风格的TTS模型调整为自发风格，方式是设计特定的填充停顿适应、节奏适应和音质适应
####### 跨语言转换声音，例如，在英语讲话者没有任何普通话语音数据的情况下，使用英语讲话者合成普通话语音。        
##### 高效适应性
###### 少量数据适应
####### 只使用少数成对的文本和语音数据，从几分钟到几秒不等
###### 少量参数适应
####### 在保证质量的同时减少参数
###### 非转录数据适应
####### 使用ASR模型转录会议中的文字，并使用转录的成对数据进行语音适配
###### zero-shot适应
####### 利用说话人编码器提取给定的参考音频的embeddings
####### 缺点：当目标说话人与源说话人有很大差异时适应质量不够好
## 未来方向
### 高质量语音合成
#### 强大的生成模型
#### 更好的特征学习
##### 半监督或无监督学习
##### 预训练学习文本/音素序列
#### 健壮性更强
##### 提高在不同领域的通用性
#### 表现力强/可控制/可转换的语音合成
#### 更像人类
##### 塑造更自然、随义、自发的风格，提高自然度
### 高效语音合成
#### 高效的数据
##### ZeroSpeech挑战
###### 只是用语音，不涉及文本和语言知识
##### 针对目标说话人适配数据少的情况
#### 高效参数
##### 设计具有更少内存占用、功耗和延迟的轻量模型
#### 节能
##### 减少TTS训练和推断中的失败，减少碳排放
### 构建完全端到端的TTS流水线
#### 减少三个步骤中的误差累积
## 评估方法
### 主观评估
#### 平均意见评分MOS（五分制）
#### 多激励隐藏参考基准测试方法MUSHRA(百分制)
#### 比较平均意见得分CMOS
##### 比较被测模型的MOS值与基线之间的差异
#### AB偏好测试
##### 要求听众比较两个模型合成的同一个句子的语音
#### ABX偏好测试
##### 要求听众指定收听三个片段ABX，X表示目标语音，AB分别是两个模型生成的语音，判断AB是在语音的个性特征方面是否更接近X，或者无法给出判断
### 客观评估
#### 直接计算生成语音的特征
##### F0
##### 音高
##### C0的均方根误差RMSE
##### 绝对误差和负对数似然NLL
##### 参考音频和预测音频的持续时间
##### 字符错误率CER
##### 合成语音的单词错误率WER
##### 话语错误率UER
#### Mel-Cepstral Distortion(MCD)
##### 计算合成音频与参考音频的mel-spectral特征之间的光谱距离来量化MFCC的重建性能
